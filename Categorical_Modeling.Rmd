---
title: "Logistic Regression"
author: "Naive Frequentists"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=F}
knitr::opts_chunk$set(echo = TRUE, message = F)
```


```{r, include=F}
library(foreign)
library(gam)
library(ggrepel)
library(ggthemes)
library(gridExtra)
library(magrittr)
library(nnet)
library(pander)
library(splines)
library(splines2)
library(tidyverse)
library(VGAM)
library(pscl)
library(vcd)
library(AER)
options(digits = 3)
theme_update(plot.title = element_text(hjust = 0.5))
```


```{r, include=F}
dat = read_csv("heart.csv")
dataset_wfactors = dat %>% mutate(anaemia = factor(anaemia),
                             diabetes = factor(diabetes),
                             high_blood_pressure = factor(high_blood_pressure),
                             sex = factor(sex),
                             smoking = factor(smoking),
                             DEATH_EVENT = factor(DEATH_EVENT)
                             )

# Dataset without factors
dataset_no_factors = dataset_wfactors %>% 
  select(-c(anaemia, diabetes, high_blood_pressure, sex, smoking, DEATH_EVENT))
```

&nbsp;


```{r}
apply(dataset_wfactors, 2, function(x) sum(is.na(x)))
```

&nbsp;

## Logistic Regression (Initial Screening)

**Assumptions:**\

1. **The outcome `Heart Failure` $\sim Binomial(1, p)$, with the sample being independent and identically distributed (IID).**\
It is reasonable to assume that patients are independent with a constant probability of an event, and that they come from the same underlying population.

&nbsp;

2. **$E\left[\textit{Heart Failure}\; |\; X\right] = P(Heart\; Failure = 1) = p$ is linear on the logit scale $\left(logit(p) = \frac{p}{1 - p}\right)$.**\
We assessed this assumption through an analysis of the residuals. We did not detect any non-linear trends for any of our 3 covariates. It follows that the linearity assumption is defensible.

&nbsp;

3. **~10-20 events (heart failure within 30 days) per covariate.**\
This assumption is met as we have 35 events and our logistic LASSO fit has 3 covariates => 11.667 events per covariate. 

&nbsp;

```{r}
logi_fit = glm(data = dataset_wfactors, DEATH_EVENT ~ ., family = binomial())
pander(summary(logi_fit))
AIC(logi_fit)
```

&nbsp;

**It appears that `age`, `ejection_fraction`, `serum_creatinine`, and `time` are the only covariates that are statistically significant at the standard 0.05 level. Before proceeding with further modeling, we will first check model diagnostics for this initial fit:**

&nbsp;

### Distribution of Fitted values and Leverage (hat-values)
```{r}
ggplot(dataset_wfactors, aes(logi_fit$fitted.values)) + geom_histogram() + ggtitle("p-hats") + xlab("p-hats")
ggplot(dataset_wfactors, aes(hatvalues(logi_fit))) + geom_histogram() + ggtitle("hat-values") + xlab("hat-values")
```

&nbsp;

### Cook's Distance
```{r}
influencePlot(logi_fit, col = "red")
```

&nbsp;

**Observations 132, 187, and 196 only have large Studentized residuals, with their leverage and Cook's D. being small. Conversely, observation 61 has a small Studentized residual and Cook's D but a large leverage value. Likewise, observation 218 has a moderate Studentized residual and large leverage and Cook's D values.**

&nbsp;

### Residual analysis
```{r}
par(mfrow = c(1, 2))
residualPlots(logi_fit)
```

&nbsp;

### Outlier Test
```{r}
outlierTest(logi_fit)
```

&nbsp;

**Not surprisingly, only observation 187 became a statistically significant outlier. This is because it has the largest studentized residual value.**

**Based on the above, it seems that 218 may be the most "influential" observation, while observation 187 is the most significant outlier. We will re-run the above analysis with these points removed and note any appreciable change in the coefficients:**

&nbsp;

### Interpretations of Beta Coefficients:

```{r}

```

&nbsp;

